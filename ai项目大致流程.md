首先，我从kaggle中找到了一个关于恶意软件分类的数据集，这是一个二元分类的数据集。

接着对它进行了数据预处理，对于数据集中的**离散特征通过频次直方图过滤低频类别**，异常值标记为缺失后用众数补充，**连续特征通过箱线图识别异常值**，异常值标记为缺失后用中位数补充，**对有序离散特征采用标签编码**，**无序离散特征采用独热编码**，**连续特征通过对数变换降低偏态分布影响**。



*这里离散特征只是过滤低频类别，并不补充缺失值*

*对离散特征通过频次直方图过滤低频类别，这一做法合理，因为低频类别可能对模型的训练影响较小，过滤掉可以减少噪声。对于有序离散特征采用标签编码，无序离散特征采用独热编码，这也是标准的做法，能够有效地将离散特征转化为模型可处理的数值形式*

*用箱线图识别异常值，并将异常值标记为缺失后用众数补充，这种方式在一定程度上可以保留数据的分布特性，避免异常值对模型训练的干扰。对连续特征通过Box-Cox变换降低偏态分布影响，Box-Cox变换是一种常用的处理偏态分布的方法，能够使数据更接近正态分布，有利于很多基于正态分布假设的模型的训练。*





**（创新点）**共有80个特征：PE 文件头的签名、大小、文件大小、特征标志、可选头的特征
接着进行特征工程时，我采用无监督学习的方法，使用了Kmeans、Kmeans++还有MinibatchKmens来获得聚类特征，其中过程：首先通过**肘部法**则将K值从小到大遍历找到肘部拐点，然后再对数据集进行降维，**先降维再聚类**，一是降低维数可以减少聚类时计算数据点间距离的开销，二是可以减少噪音和冗余特征。降维时使用了两种降维方法，一种是线性降维PCA，一种是非线性降维t-SNE，最后计算两个降维后各三种聚类方法总计六种方法的轮廓系数，最终选择**轮廓系数大**的那个组合得到了新特征。



*线性降维PCA：通过最大化数据方差（协方差矩阵）（信息保留），将高维数据投影到低维空间。*

*非线性降维t-SNE：通过概率分布匹配，将高维数据的局部相似性保留到低维空间（适合可视化）。*

*轮廓系数：它结合了**簇内凝聚度**（样本与同簇其他样本的相似性）和**簇间分离度**（样本与其他簇样本的差异性），取值范围为 [-1, 1]。*

*每次运行结果可能不同，导致聚类特征不稳定。若将其用于特征工程，需固定随机种子（`random_state`）以确保可复现性。*



**（创新点）**
得到特征后，接下来就是项目模型的训练，将整个数据集按8：2划分为训练集和测试集

对于少数类样本采用半监督学习来进行数据增强（kaggle中测试集则不提供标签，参赛者需要使用训练好的模型对测试集进行预测，并提交预测结果，对于这些没有标签的测试集可以取小部分数据进行半监督学习），**以此增加数据的多样性，提高模型的鲁棒性和泛化能力**。具体做法是，使用训练集训练一个初步模型，我采用的是**随机森林**，训练好后对无标签数据进行预测得到伪标签和其对应的**置信度（基于分类模型的概率输出（如 Softmax））**，然后对**伪标签的置信度筛选**，只有在0.95以上的标签才认可，然后再将这个数据加入到原来数据集中重新训练模型，多次后（虽然数据集不大，但是为了节省时间也只循环训练了10次）将得到的伪数据加入训练集，至此数据增强结束。

划分后对于训练集应用**SMOTE进行过采样**。

处理数据不平衡结束后采用多个模型进行训练，在训练每个基础模型时，先进行**网格搜索优化超参数**，再使用最优参数训练最终模型。包括支持向量机、随机森林、XGboost、LightGBM，并计算各个模型交叉验证后的所有fold的F1 score均值，以及roc曲线下的auc值的均值。



*交叉验证：**将数据集多次划分为训练集和验证集，重复训练模型并评估性能，最终取平均结果作为模型的泛化能力估计。**通过循环使用不同子集训练和验证，交叉验证能更充分利用数据，避免过拟合或欠拟合的误判。*

***K 折交叉验证（K-Fold CV）**：*

1. *将数据集随机划分为 K 个互不重叠的子集（折，Fold）。*
2. *每次用 \(K-1\) 折训练，剩余 1 折验证，重复 K 次。*
3. *计算 K 次验证结果的平均值（如准确率、均方误差）。*



**（创新点）**
训练过基础模型后，又设计了一个stacking集成框架，基模型选择LightGBM、XGBoost、Randomforest，元模型选择LightGBM，结合加权融合策略（怎么赋值权重），最终auc值达到0.82，显著优于任一单一模型。

对比实验：通过增加了SMOTE过采样、数据增强、模型优化后的模型，这些创新点使得F1-score提高了13%。

最后通过SHAP库可视化特征贡献度，探究特征重要性。